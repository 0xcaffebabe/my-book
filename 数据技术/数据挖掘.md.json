{"content":"# 数据挖掘\n\n## 推荐\n\n- 基于用户的协作型过滤 为用户计算与其他用户的相似度\n- 基于物品的协作型过滤 为物品计算与其他物品的相似度 需要提前计算好物品的相似集合\n\n### 收集偏好\n\n不管偏好是评价何物品 最终都需要一种方法将其对应到数字：\n\n使用这一的一个item表示某一用户的偏好：\n\n```py\n'Toby': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0, 'Superman Returns': 4.0}\n```\n\n### 寻找相似的用户\n\n欧几里得距离评价:\n\n![屏幕截图 2020-10-11 110258](/assets/屏幕截图%202020-10-11%20110258.png)\n\n通过将用户转换为坐标中的点 计算点之间的距离 距离越近越相似\n\n```py\ndef sim_distance(prefs, person1, person2):\n    # 得到shared_items的列表\n    si = {}\n    for item in prefs[person1]:\n        if item in prefs[person2]:\n            si[item] = 1\n\n    # 如果两者没有共同之处，则返回0\n    if len(si) == 0:\n        return 0\n\n    # 计算所有差值的平方和\n    sum_of_squares = 0\n    for item in si.keys():\n        # 这里所做的就是等同于在计算二维中x y 的差值\n        sum_of_squares = sum_of_squares + pow(prefs[person1][item] - prefs[person2][item], 2)\n\n    return 1 / (1 + sqrt(sum_of_squares))\n```\n\n皮尔逊相关度评价：\n\n判断两组数据与某一直线拟合程度的度量\n\n![屏幕截图 2020-10-11 112937](/assets/屏幕截图%202020-10-11%20112937.png)\n\n```py\ndef sim_pearson(prefs, p1, p2):\n    # 得到双方都曾评价过的物品列表\n    si = {}\n    for item in prefs[p1]:\n        if item in prefs[p2]:\n            si[item] = 1\n\n    # 如果两者没有共同之处，则返回0\n    if len(si) == 0:\n        return 0\n\n    # 得到列表元素的个数\n    n = len(si)\n\n    # 对所有偏好求和\n    sum1 = sum([prefs[p1][it] for it in si])\n    sum2 = sum([prefs[p2][it] for it in si])\n\n    # 求平方和\n    sum1Sq = sum([pow(prefs[p1][it], 2) for it in si])\n    sum2Sq = sum([pow(prefs[p2][it], 2) for it in si])\n\n    # 求乘积之和\n    pSum = sum([prefs[p1][it] * prefs[p2][it] for it in si])\n\n    # 计算皮尔逊评价值\n    num = pSum - (sum1 * sum2 / n)\n    den = sqrt((sum1Sq - pow(sum1, 2) / n) * (sum2Sq - pow(sum2, 2) / n))\n    if den == 0:\n        return 0\n\n    r = num / den\n\n    return r\n```\n\n### 为某一用户找出与其相似的用户\n\n```py\ndef topMatches(prefs, person, n=5, similarity=sim_pearson):\n    scores = [(similarity(prefs, person, other), other)\n              for other in prefs if other != person]\n    scores.sort()\n    scores.reverse()\n    return scores[0:n]\n```\n\n### 根据相似用户推荐物品\n\n```py\ndef getRecommendations(prefs, person, similarity=sim_pearson):\n    totals = {}\n    simSums = {}\n    for other in prefs:\n        # 不和自己比较\n        if other == person:\n            continue\n        sim = similarity(prefs, person, other)\n\n        # 忽略相似度小于等于0的用户\n        if sim <= 0: continue\n        for item in prefs[other]:\n            # 只对自己还未评价过的电影推荐\n            if item not in prefs[person] or prefs[person][item] == 0:\n                # 计算某部影片的分值\n                totals.setdefault(item, 0)\n                totals[item] += prefs[other][item] * sim\n                # 每部影片的评价人相似度总和\n                simSums.setdefault(item, 0)\n                simSums[item] += sim\n\n    # 计算一个归一化列表\n    rankings = [(total / simSums[item], item) for item, total in totals.items()]\n\n    # Return the sorted list\n    rankings.sort()\n    rankings.reverse()\n    return rankings\n```\n\n### 匹配物品\n\n通过将物品与人交换 可以使用查找相似的用户的算法来查找相似的物品\n\n```py\ndef transformPrefs(prefs):\n    result = {}\n    for person in prefs:\n        for item in prefs[person]:\n            result.setdefault(item, {})\n\n            # Flip item and person\n            result[item][person] = prefs[person][item]\n    return result\n```\n\n### 构造物品相似集合\n\n```py\ndef calculateSimilarItems(prefs, n=10):\n    # 建立一个key为物品 value为与其相似相近物品的的字典\n    result = {}\n\n    # 反转物品与人\n    itemPrefs = transformPrefs(prefs)\n    c = 0\n    for item in itemPrefs:\n        # 针对大数据更新状态变量\n        c += 1\n        if c % 100 == 0:\n            print(\"%d / %d\" % (c, len(itemPrefs)))\n        # 寻找最为相近的物品\n        scores = topMatches(itemPrefs, item, n=n, similarity=sim_distance)\n        result[item] = scores\n    return result\n```\n\n### 使用提前构造好的数据集获得推荐\n\n```py\ndef getRecommendedItems(prefs, itemMatch, user):\n    userRatings = prefs[user]\n    scores = {}\n    totalSim = {}\n    # 遍历当前用户评价过的物品\n    for (item, rating) in userRatings.items():\n\n        # 遍历与当前物品相近的物品\n        for (similarity, item2) in itemMatch[item]:\n\n            # 忽略已经被评价过的物品\n            if item2 in userRatings: continue\n            # 分数是通过相似度*评分\n            scores.setdefault(item2, 0)\n            scores[item2] += similarity * rating\n            # Sum of all the similarities\n            totalSim.setdefault(item2, 0)\n            totalSim[item2] += similarity\n\n    # Divide each total score by total weighting to get an average\n    rankings = [(score / totalSim[item], item) for item, score in scores.items()]\n\n    # Return the rankings from highest to lowest\n    rankings.sort()\n    rankings.reverse()\n    return rankings\n```\n\n## 群组\n\n- 监督学习：利用样本和期望输出来学习如何预测\n- 无监督学习：在一组数据中寻找某种结构\n\n### 分级聚类\n\n不断将最为相似的群组两两合并\n\n![屏幕截图 2020-10-13 103115](/assets/屏幕截图%202020-10-13%20103115.png)\n\n使用树状图来可视化聚类结果：\n\n![屏幕截图 2020-10-13 103218](/assets/屏幕截图%202020-10-13%20103218.png)\n\n通过距离来体现各元素的相似度\n\n计算两个数字列表的相关度：\n\n```py\ndef pearson(v1, v2):\n    # 简单求和\n    sum1 = sum(v1)\n    sum2 = sum(v2)\n\n    # 平方根和\n    sum1Sq = sum([pow(v, 2) for v in v1])\n    sum2Sq = sum([pow(v, 2) for v in v2])\n\n    # 乘积之和\n    pSum = sum([v1[i] * v2[i] for i in range(len(v1))])\n\n    # Calculate r (Pearson score)\n    num = pSum - (sum1 * sum2 / len(v1))\n    den = sqrt((sum1Sq - pow(sum1, 2) / len(v1)) * (sum2Sq - pow(sum2, 2) / len(v1)))\n    if den == 0: return 0\n\n    return 1.0 - num / den\n```\n\n代表一个聚类节点：\n\n```py\nclass bicluster:\n    def __init__(self, vec, left=None, right=None, distance=0.0, id=None):\n        self.left = left\n        self.right = right\n        self.vec = vec\n        self.id = id\n        self.distance = distance\n```\n\n### 列聚类\n\n将矩阵转置\n\n```py\ndef rotatematrix(data):\n    newdata = []\n    for i in range(len(data[0])):\n        newrow = [data[j][i] for j in range(len(data))]\n        newdata.append(newrow)\n    return newdata\n```\n\n可以得到单词的聚类结果\n\n### K-均值聚类\n\n1. 随机确定k个中心位置\n2. 将各个数据项分配个最近的中心点\n3. 将中心点移动到各个节点的平均位置\n4. 重复2-3 直到不再变化\n\n```py\ndef kcluster(rows, distance=pearson, k=4):\n    # 确定每个点的最大最小值\n    ranges = [(min([row[i] for row in rows]), max([row[i] for row in rows]))\n              for i in range(len(rows[0]))]\n\n    # 随机创建k个中心点\n    clusters = [[random.random() * (ranges[i][1] - ranges[i][0]) + ranges[i][0]\n                 for i in range(len(rows[0]))] for j in range(k)]\n\n    lastmatches = None\n\n    for t in range(100):\n        dis_sum = 0\n        print('Iteration %d' % t)\n        bestmatches = [[] for i in range(k)]\n\n        # 在每一行中寻找距离最近的中心点\n        for j in range(len(rows)):\n            row = rows[j]\n            bestmatch = 0\n            for i in range(k):\n                d = distance(clusters[i], row)\n                if d < distance(clusters[bestmatch], row):\n                    bestmatch = i\n            dis_sum += d\n            bestmatches[bestmatch].append(j)\n\n        # 如果与上次结果相同 则结束\n        if bestmatches == lastmatches:\n            break\n        lastmatches = bestmatches\n\n        # 把中心移到其所有成员的平均位置处\n        for i in range(k):\n            avgs = [0.0] * len(rows[0])\n            if len(bestmatches[i]) > 0:\n                for rowid in bestmatches[i]:\n                    for m in range(len(rows[rowid])):\n                        avgs[m] += rows[rowid][m]\n                for j in range(len(avgs)):\n                    avgs[j] /= len(bestmatches[i])\n                clusters[i] = avgs\n\n    return bestmatches, dis_sum\n```\n\n### 二维形式展示\n\n在一个二维平面 通过不同数据项的距离来计算得到一个二维平面图\n\n```py\ndef scaledown(data, distance=pearson, rate=0.01):\n    n = len(data)\n\n    # 每一对数据项之间的距离\n    realdist = [[distance(data[i], data[j]) for j in range(n)]\n                for i in range(0, n)]\n\n    # 随机初始化节点再二维空间的起始位置\n    loc = [[random.random(), random.random()] for i in range(n)]\n    fakedist = [[0.0 for j in range(n)] for i in range(n)]\n\n    lasterror = None\n    for m in range(0, 1000):\n        # 寻找投影后的距离\n        for i in range(n):\n            for j in range(n):\n                fakedist[i][j] = sqrt(sum([pow(loc[i][x] - loc[j][x], 2)\n                                           for x in range(len(loc[i]))]))\n\n        # 移动节点\n        grad = [[0.0, 0.0] for i in range(n)]\n\n        totalerror = 0\n        for k in range(n):\n            for j in range(n):\n                if j == k: continue\n                # The error is percent difference between the distances\n                errorterm = (fakedist[j][k] - realdist[j][k]) / realdist[j][k]\n\n                # Each point needs to be moved away from or towards the other\n                # point in proportion to how much error it has\n                grad[k][0] += ((loc[k][0] - loc[j][0]) / fakedist[j][k]) * errorterm\n                grad[k][1] += ((loc[k][1] - loc[j][1]) / fakedist[j][k]) * errorterm\n\n                # Keep track of the total error\n                totalerror += abs(errorterm)\n        print(totalerror)\n\n        # If the answer got worse by moving the points, we are done\n        if lasterror and lasterror < totalerror: break\n        lasterror = totalerror\n\n        # Move each of the points by the learning rate times the gradient\n        for k in range(n):\n            loc[k][0] -= rate * grad[k][0]\n            loc[k][1] -= rate * grad[k][1]\n\n    return loc\n```\n\n## 搜索与排名\n\n### 建立索引\n\n![屏幕截图 2020-10-19 153923](/assets/屏幕截图%202020-10-19%20153923.png)\n\n### 分词\n\n从html结构中获取文本节点，对其分词\n\n```py\ndef gettextonly(self, soup):\n    v = soup.string\n    if len(v) == 0:\n        c = soup.contents\n        resulttext = ''\n        for t in c:\n            subtext = self.gettextonly(t)\n            resulttext += subtext + '\\n'\n        return resulttext\n    else:\n        return v.strip()\n\n# 使用正则表达式进行分词\n@staticmethod\ndef separatewords(self, text):\n    splitter = re.compile('\\\\W*')\n    return [s.lower() for s in splitter.split(text) if s != '']\n```\n\n### 查询\n\n1. 进行分词\n2. 查找分出的词的响应ID\n3. 根据这些词来查找相关url\n\n```py\ndef getmatchrows(self, q):\n    # 构造sql查询条件字符串\n    fieldlist = 'w0.urlid'\n    tablelist = ''\n    clauselist = ''\n    wordids = []\n    # 分词\n    words = q.split(' ')\n    tablenumber = 0\n    for word in words:\n        # 获取单词的ID\n        wordrow = self.con.execute(\n            \"select rowid from wordlist where word='%s'\" % word).fetchone()\n        if wordrow is not None:\n            wordid = wordrow[0]\n            wordids.append(wordid)\n            if tablenumber > 0:\n                tablelist += ','\n                clauselist += ' and '\n                clauselist += 'w%d.urlid=w%d.urlid and ' % (tablenumber - 1, tablenumber)\n            fieldlist += ',w%d.location' % tablenumber\n            tablelist += 'wordlocation w%d' % tablenumber\n            clauselist += 'w%d.wordid=%d' % (tablenumber, wordid)\n            tablenumber += 1\n    # 根据条件进行查询\n    fullquery = 'select %s from %s where %s' % (fieldlist, tablelist, clauselist)\n    print(fullquery)\n    cur = self.con.execute(fullquery)\n    rows = [row for row in cur]\n    return rows, wordids\n```\n\n### 基于内容的排名\n\n使用一个归一化函数将结果映射到0-1之间：\n\n```py\ndef normalizescores(self, scores, smallIsBetter=0):\n    vsmall = 0.00001  # 避免除0\n    if smallIsBetter:\n        minscore = min(scores.values())\n        return dict([(u, float(minscore) / max(vsmall, l)) for (u, l) in scores.items()])\n    else:\n        maxscore = max(scores.values())\n        if maxscore == 0: maxscore = vsmall\n        return dict([(u, float(c) / maxscore) for (u, c) in scores.items()])\n```\n\n#### 单词频度\n\n根据单词在网页中出现的次数对网页进行评价\n\n```py\ndef frequencyscore(self, rows):\n    counts = dict([(row[0], 0) for row in rows])\n    for row in rows: counts[row[0]] += 1\n    return self.normalizescores(counts)\n```\n\n#### 文档位置\n\n根据单词离文档首部的距离进行评价\n\n```py\ndef locationscore(self, rows):\n    locations = dict([(row[0], 1000000) for row in rows])\n    for row in rows:\n        loc = sum(row[1:])\n        if loc < locations[row[0]]: locations[row[0]] = loc\n    return self.normalizescores(locations, smallIsBetter=1)\n```\n\n#### 单词距离\n\n单词间距更近 得分越高\n\n```py\ndef distancescore(self, rows):\n    # 只有一个单词 则得分一样\n    if len(rows[0]) <= 2: return dict([(row[0], 1.0) for row in rows])\n    # 初始化分数 很大\n    mindistance = dict([(row[0], 1000000) for row in rows])\nfor row in rows:\n        dist = sum([abs(row[i] - row[i - 1]) for i in range(2, len(row))])\n        if dist < mindistance[row[0]]: mindistance[row[0]] = dist\n    return self.normalizescores(mindistance, smallIsBetter=1)\n```\n\n### 使用外部回指链接\n\n#### 简单计数\n\n统计其他网页链接本网页的个数 个数越多 评分越高\n\n```py\ndef inboundlinkscore(self, rows):\n    uniqueurls = dict([(row[0], 1) for row in rows])\n    inboundcount = dict(\n        [(u, self.con.execute('select count(*) from link where toid=%d' % u).fetchone()[0]) for u in uniqueurls])\n    return self.normalizescores(inboundcount)\n```\n\n#### PageRank\n\n为所有的网页设置一个默认PR值 ，每个网页的PR值计算公式：\n\n![屏幕截图 2020-10-20 112243](/assets/屏幕截图%202020-10-20%20112243.png)\n\nPR(A) = 0.15 + 0.85 * (PR(B)/links(B) + PR(C)/links(C) + PR(D)/links(D))\n\n#### 使用链接文本\n\n根据指向网页的链接文本来评价该网页\n\n```py\ndef linktextscore(self, rows, wordids):\n    linkscores = dict([(row[0], 0) for row in rows])\n    for wordid in wordids:\n        cur = self.con.execute(\n            'select link.fromid,link.toid from linkwords,link where wordid=%d and linkwords.linkid=link.rowid' % wordid)\n        for (fromid, toid) in cur:\n            if toid in linkscores:\n                pr = self.con.execute('select score from pagerank where urlid=%d' % fromid).fetchone()[0]\n                linkscores[toid] += pr\n    maxscore = max(linkscores.values())\n    if maxscore == 0:\n        maxscore = 0.00001\n    normalizedscores = dict([(u, float(l) / maxscore) for (u, l) in linkscores.items()])\n    return normalizedscores\n```\n","commitList":[{"date":"2021-10-08T12:45:10+08:00","author":"cjiping","message":"doc开发期 构建期调整","hash":"704efe7455b7fb2fbd81ec60099191ff982e639a"},{"date":"2021-10-07T19:38:58+08:00","author":"My","message":"init","hash":"4e0456332231ba0523aa526415f9982377358870"}],"hasMoreCommit":false,"totalCommits":0}