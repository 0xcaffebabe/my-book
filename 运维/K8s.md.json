{"content":"# Kubernetes\n\n>Kubernetes（常简称为K8s）是用于自动部署、扩展和管理容器化（containerized）应用程序的开源系统\n\n![屏幕截图 2020-09-07 145646](/assets/屏幕截图%202020-09-07%20145646.png)\n\n## 架构\n\n![屏幕截图 2020-09-07 145831](/assets/屏幕截图%202020-09-07%20145831.png)\n\nmaster：用于控制集群\n\n- API服务器：外部访问入口\n- Scheduler：调度应用（为应用分配工作节点）\n- Controller Manager：执行集群级别的功能\n- etcd：存储集群配置的分布式数据存储\n\n工作节点：运行用户部署应用的节点\n\n- 容器运行时：Docker 或者其他容器\n- Kubelet：与API服务器通信 管理当前节点的容器\n- kube-proxy:负责组件之间的负载均衡\n\n### 分布式\n\n- Kubenetes系统组件间只能通过API服务器通信\n- 为了保证高可用性， master的每个组件可以有多个实例\n\n### etcd\n\n只有API服务器才能直接与etcd通信\n\n数据在etcd中存储的是一个层次级目录结构 末端节点存储的json数据\n\n集群一致性保证：raft算法\n\n### API 服务器\n\n- 认证授权\n\n![屏幕截图 2020-09-15 143004](/assets/屏幕截图%202020-09-15%20143004.png)\n\n- 通知客户端资源变更\n\n![屏幕截图 2020-09-15 143259](/assets/屏幕截图%202020-09-15%20143259.png)\n\n#### 安全防护\n\n- pod 使用 service accounts机制进行认证\n\n![屏幕截图 2020-09-16 135815](/assets/屏幕截图%202020-09-16%20135815.png)\n\n```sh\nkubectl get sa # 获取服务账户\nkubectl create serviceaccount foo # 创建\n```\n\n![屏幕截图 2020-09-16 140540](/assets/屏幕截图%202020-09-16%20140540.png)\n\n- 使用sa:\n\n```yaml\nspec:\n  serviceAccountName: foo\n```\n\nRBAC控制：使用插件\n\n### 调度器\n\n利用 API 服务器的监听机制等待新创建的 pod, 然后给每个新的、 没有节点集的 pod 分配节点\n\n![屏幕截图 2020-09-15 143719](/assets/屏幕截图%202020-09-15%20143719.png)\n\n调度过程是很复杂的：\n\n- 选择可用节点\n- 选择最佳节点\n- 高级调度\n  - 如何保证节点副本分布尽可能均匀\n\n### 控制管理器\n\n确保系统真实状态朝 API 服务器定义的期望的状态收敛\n\n- rc rs控制器 deployment控制器...\n\n### Kubelet\n\n- 在 API 服务器中创建Node 资源, 等待pod分配给它并启动pod\n- 向API服务器提供监控\n- 当pod从 API服务器删除, kubelet也会删除pod\n\n![屏幕截图 2020-09-15 150145](/assets/屏幕截图%202020-09-15%20150145.png)\n\n### kube-proxy\n\n确保用户可以访问后端的pod\n\n两种模式：\n\n![屏幕截图 2020-09-15 150639](/assets/屏幕截图%202020-09-15%20150639.png)\n![屏幕截图 2020-09-15 150657](/assets/屏幕截图%202020-09-15%20150657.png)\n\n### 控制器协作\n\n![屏幕截图 2020-09-15 151627](/assets/屏幕截图%202020-09-15%20151627.png)\n\n### pod 到底是什么\n\n![屏幕截图 2020-09-15 152859](/assets/屏幕截图%202020-09-15%20152859.png)\n\n### 网络\n\n![屏幕截图 2020-09-15 153101](/assets/屏幕截图%202020-09-15%20153101.png)\n\n相同节点的pod通信：\n\n![屏幕截图 2020-09-15 153836](/assets/屏幕截图%202020-09-15%20153836.png)\n\n不同节点的pod通信：\n\n![屏幕截图 2020-09-15 153900](/assets/屏幕截图%202020-09-15%20153900.png)\n\n只有当所有节点连接到相同网关的时候 上述方案才有效\n\n### 服务的实现\n\n服务暴露的外部ip与端口通过每个节点上的kube-proxy实现\n\n暴露的这个ip是虚拟的 主要是用来做映射用的 当kube-proxy接收到这个ip的请求 就会查找映射 转发请求\n\n![屏幕截图 2020-09-15 154607](/assets/屏幕截图%202020-09-15%20154607.png)\n\n### 高可用集群\n\n应用高可用：\n\n- 水平扩展\n- 主从架构\n\nmaster高可用：\n\n![屏幕截图 2020-09-15 154955](/assets/屏幕截图%202020-09-15%20154955.png)\n\n- etcd自身会进行数据同步\n- API 服务器是无状态的\n- 控制器与调度器会进行主从选举 只有leader才会进行调度控制工作\n\n## 优点\n\n- 简化部署\n- 充分利用硬件\n- 健康检查 自修复\n- 自动扩容\n\n## 在K8S中运行应用\n\n根据描述信息生成对应的pod 在pod中运行容器\n\nK8S会保证集群中的容器数量实例 在容器死亡时 会启动新容器替补\n\nK8S 在运行时可根据需求动态调整副本数量\n\n通过kube-proxy能进行服务连接动态切换\n\n### 本地运行K8S\n\n- 安装minikube\n- 安装kubectl\n\n```sh\nminikube start \\\n--image-mirror-country=cn \\\n--registry-mirror='https://t9ab0rkd.mirror.aliyuncs.com' \\\n--image-repository='registry.cn-hangzhou.aliyuncs.com/google_containers'\n```\n\n### 部署第一个应用\n\n```sh\nkubectl  run  kubia  --image=luksa/kubia  --port=8080  # 创建容器运行\nkubectl get pods # 获取pod\nkubectl get rc\nkubectl port-forward kubia 8080:8080 # 开启端口转发\nkubectl get pods -o wide # 查看应用在哪个节点\nkubectl scale rc kubia --replicas=3 # 水平扩容\n```\n\n![屏幕截图 2020-09-08 140428](/assets/屏幕截图%202020-09-08%20140428.png)\n\n逻辑架构：\n\n![屏幕截图 2020-09-08 142015](/assets/屏幕截图%202020-09-08%20142015.png)\n\n- RC用来确保始终有pod运行\n- 使用http服务来完成外部请求到pod的映射\n\n## pod\n\n一组紧密相关的容器 独立的逻辑机器\n\n![屏幕截图 2020-09-08 135241](/assets/屏幕截图%202020-09-08%20135241.png)\n\n一 个 pod 中的所有容器都在相同的 network 和 UTS 命名空间下运行\n\n每个 pod 都有自己的 IP 地址， 并且可以通过这个专门的网络实现 pod\n之间互相访问\n\npod的使用：\n\n- 倾向于单个pod单个容器\n\n### 使用yml创建pod\n\n```yml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-manual\n  labels:\n    env: test # 指定一个标签\nspec:\n  nodeSelector: # 选择特定标签的节点\n    super: \"true\"\n  containers:\n  - image: luksa/kubia\n    name: kubia\n    ports:\n    - containerPort: 8080\n      protocol: TCP\n```\n\n```sh\nkubectl create -f kubia-manual.yaml\nkubectl logs kubia-manual # 查看日志\n```\n\n### 标签\n\n```sh\nkubectl get po --show-labels\nkubectl label po kubia-manual createtion_method=manual # 修改标签\nkubectl label node minikube super=true\nkubectl get po -l createtion_method=manual # 根据标签筛选\n```\n\n### 注解\n\n注解也是键值对\n\n```sh\nkubectl annotate pod kubia-manual wang.ismy/name=\"cxk\"\n```\n\n### 命名空间\n\n命名空间简单为对象划分了一个作用域\n\n```sh\nkubectl get ns\nkubectl get po -n kube-system # 获取命名空间下的pod\n\nkubectl create namespace custom-namespace # 创建命名空间\nkubectl create -f kubia-manual.yaml -n custom-namespace # 指定命名空间\n```\n\n### 停止与移除\n\n```sh\nkubectl delete po kubia-manual # 根据名字删除\n```\n\n## 副本机制\n\nk8s 会保证 pod 以及 容器的健康运行\n\n### 存活探针\n\n当存活探针探测失败 容器就会重启\n\n- 创建\n\n```yml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-liveness\nspec:\n  containers:\n  - image: luksa/kubia-unhealthy\n    name: kubia\n    livenessProbe: # 存活探针\n      httpGet: # 返回2xx 或者 3xx就代表活着\n        path: /\n        port: 8080\n```\n\n### ReplicationController\n\n创建和管理一个pod的多个副本\n\n![屏幕截图 2020-09-09 164430](/assets/屏幕截图%202020-09-09%20164430.png)\n\n- 创建\n\n```yml\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: kubia\nspec:\n  replicas: 3\n  selector:\n    app: kubia\n  template:\n    metadata:\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - name: kubia\n        image: luksa/kubia\n        ports:\n        - containerPort: 8080\n```\n\n控制器通 过 创建 一 个新的替代pod来响应pod的删除操作\n\n通过更改标签的方式来实现rc与pod的关联\n\n- 扩容\n\n```sh\nkubectl scale rc kubia --replicas=10\n```\n\n- 删除\n\n```sh\nkubectl delete rc kubia\n```\n\n### ReplicaSet\n\nReplicaSet 会 替代 rc\n\nrs 的pod 选择器的表达能力更强\n\n- 创建\n\n```yml\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: kubia\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia\n  template:\n    metadata:\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - name: kubia\n        image: luksa/kubia\n```\n\n### DaemonSet\n\n由DaemonSet 创建的pod 会绕过调度程序 会在所有集群节点上运行（或者也可以通过指定`nodeSelector`在其他节点运行）\n\n![屏幕截图 2020-09-09 191240](/assets/屏幕截图%202020-09-09%20191240.png)\n\n- 创建\n\n```yml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ssd-monitor\nspec:\n  selector:\n    matchLabels:\n      app: ssd-monitor\n  template:\n    metadata:\n      labels:\n        app: ssd-monitor\n    spec:\n      nodeSelector:\n        disk: ssd\n      containers:\n      - name: main\n        image: luksa/ssd-monitor\n```\n\n### Job\n\n允许运行 一 种 pod, 该 pod 在内部进程成功结束时， 不重启容器。\n\n- 创建\n\n```yml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: batch-job\nspec:\n  completions: 5 # 运行pod数\n  parallelism: 2 # 并行运行数\n  template:\n    metadata:\n      labels:\n        app: batch-job\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: main\n        image: luksa/batch-job\n```\n\n### CronJob\n\n- 创建\n\n```yml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-job\nspec:\n  schedule: \"0,15,30,45 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            app: batch-job\n        spec:\n          restartPolicy: OnFailure\n          containers:\n          - name: main\n            image: luksa/batch-job\n```\n\n## 服务\n\n是一种为一组功能相同的 pod 提供单一不变的接入点的资源\n\n![屏幕截图 2020-09-10 190129](/assets/屏幕截图%202020-09-10%20190129.png)\n\n- 创建\n\n```yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: kubia\n```\n\n### 服务间的发现\n\n- 通过环境变量\n\n```sh\nkubectl exec kubia-9knkg -- env\n```\n\n- 通过DNS\n\n域名：kubia.default.svc.cluster.local\n\n如果在同一命名空间下 直接使用 kubia即可\n\n### Endpoint\n\n暴露一个服务的 IP 地址和端口的列表\n\n```sh\nkubectl get endpoints kubia\n```\n\n### 暴露服务给外部\n\n- NodePort：每个集群节点都会在节点上打开一个端口 将在该端口上接收到的流量重定向到基础服务\n\n```java\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia-nodeport\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 8080\n    nodePort: 30123\n  selector:\n    app: kubia\n```\n\n通过nodeip:30123 访问\n\n- 负载均衡器将流量重定向到跨所有节点的节点端口。客户端通过负载均衡器的 IP 连接到服务\n\n```yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia-loadbalancer\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: kubia\n```\n\n通过externalip:一个随机端口访问\n\n- Ingress 只需要 一 个公网 IP 就能为许多服务提供访问\n\n启用：\n\n```sh\nminikube addons enable ingress\n```\n\n### 就绪探针\n\n- 创建\n\n```yaml\n# kubia-rc.yaml\n    spec:\n      containers:\n      - name: kubia\n        image: luksa/kubia\n        readinessProbe:\n          exec:\n            command:\n            - ls\n            - /var/ready # 该文件存在 容器才被认为就绪\n```\n\n### 服务故障排除\n\n- 确保从集群内连接到服务的集群IP\n- 服务的集群IP 是虚拟IP, 是无法ping通的\n- 如果已经定义了就绪探针， 请确保 它返回成功；否则该pod不会成为服务的一部分\n- 确认某个容器是服务的 一 部分\n- 检查是否连接到服务公开的端口，而不是目标端口\n- 尝试直接连接到podIP以确认pod正在接收正确端口上的 连接\n- 法通过pod的IP 访问应用， 请确保应用不是仅绑定 到本地主机\n\n## 卷\n\n卷是 pod 的 一 个组成部分， 因此像容器 一 样在 pod 的规范中定义\n\n![屏幕截图 2020-09-12 112125](/assets/屏幕截图%202020-09-12%20112125.png)\n![屏幕截图 2020-09-12 112142](/assets/屏幕截图%202020-09-12%20112142.png)\n\n### 在容器之间共享数据\n\nemptyDir：pod被删除时 卷的内容就会丢失\n\n- 创建\n\n```yml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fortune\nspec:\n  containers:\n  - image: luksa/fortune\n    name: html-genrator\n    volumeMounts:\n    - name: html\n      mountPath: /var/htdocs\n  - image: nginx:alpine\n    name: web-server\n    volumeMounts:\n    - name: html # 使用html卷\n      mountPath: /usr/share/nginx/html # 挂载到容器的位置\n      readOnly: true\n    ports:\n    - containerPort: 80\n      protocol: TCP\n  volumes: # 创建一个卷\n  - name: html\n    emptyDir: {}\n```\n\ngitRepo：以git仓库文件填充目录文件\n\n```yml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gitrepo-volume-pod\nspec:\n  containers:\n  - image: nginx:alpine\n    name: web-server\n    volumeMounts:\n    - name: html\n      mountPath: /usr/share/nginx/html\n      readOnly: true\n    ports:\n    - containerPort: 80\n      protocol: TCP\n  volumes:\n  - name: html\n    gitRepo:\n      repository: https://github.com/luksa/kubia-website-example.git\n      revision: master\n      directory: .\n```\n\n### 访问工作节点文件\n\nhostPath 卷指向节点文件系统上的特定文件或目录\n\n### 持久化存储\n\n- gce持久盘\n- aws弹性块存储\n- nfs卷\n\n### 持久卷\n\n![屏幕截图 2020-09-12 140458](/assets/屏幕截图%202020-09-12%20140458.png)\n![屏幕截图 2020-09-12 144057](/assets/屏幕截图%202020-09-12%20144057.png)\n\n- 创建持久卷\n\n```yml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: mongodb-pv\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n    - ReadOnlyMany\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /tmp/mongodb\n```\n\n- 创建持久卷声明\n\n```yml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mongodb-pvc\nspec:\n  resources:\n    requests:\n      storage: 1Gi\n  accessModes:\n  - ReadWriteOnce\n  storageClassName: \"\" # 动态持久卷\n```\n\n- 容器使用持久卷\n\n```yml\n# ...\n  volumes:\n  - name: mongodb-data\n    persistentVolumeClaim:\n      claimName: mongodb-pvc\n```\n\n### 动态持久卷\n\n- 创建StorageClass\n\n```yml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast\nprovisioner: k8s.io/minikube-hostpath\nparameters:\n  type: pd-ssd\n```\n\n声明是通过名称引用它的 方便之处主要是在不同集群之间移植\n\n![屏幕截图 2020-09-12 150052](/assets/屏幕截图%202020-09-12%20150052.png)\n\n## 参数配置\n\n通过定义传递参数：\n\n```yml\n  - image: luksa/fortune:args\n    args: [\"2\"]\n```\n\n使用环境变量：\n\n```yml\n- image: luksa/fortune:env\n  env:\n  - name: INTERVAL\n    value: \"30\"\n```\n\n### ConfigMap\n\n类似于配置中心：\n\n![屏幕截图 2020-09-13 142528](/assets/屏幕截图%202020-09-13%20142528.png)\n\n- 创建\n\n```sh\nkubectl create configmap fortunes-config --from-literal=sleep-interval=25\n```\n\n- 单个环境变量使用\n\n```yml\n  - image: luksa/fortune:env\n    env:\n    - name: INTERVAL\n      valueFrom:\n        configMapKeyRef:\n          name: fortunes-config\n          key: sleep-interval\n```\n\n- 一次传递所有环境变量\n\n```yml\n  - image: luksa/fortune:env\n    env:\n    envFrom:\n    - prefix: CONFIG_\n    configMapRef:\n      name: fortunes-config\n    args: [\"${CONFIG_xxx}\"] # 传递到命令行\n```\n\n- 挂载到卷\n\n```yml\nvolumes:\n- name: config\n  configMap:\n    name: configmap\n```\n\n- 更新配置\n\n```sh\nkubectl edit configmap xxx\n```\n\n### Secret\n\n存储与分发敏感信息\n\n- 创建\n\n```sh\n kubectl create secret generic fortune-https --from-file=https.key\n```\n\n- 挂载卷使用\n\n```yml\n- image: xxx\n  volumeMounts:\n  - name: keys\n    mountPath: /etc/nginx/keys/\nvolumes:\n- name: keys\n  secret:\n    secretName: fortune-https\n```\n\n- 环境变量使用\n\n```yml\nenv:\n- name: FOO_SECRET\n  valueFrom:\n    secretKeyRef:\n      name: fortune-https\n      key: name\n```\n\n## pod 元数据访问\n\n### Downward API\n\n![屏幕截图 2020-09-13 152325](/assets/屏幕截图%202020-09-13%20152325.png)\n\n通过环境变量：\n\n```yml\nenv:\n- name: POD IP\n  valueFrom:\n    fieldRef:\n      fieldPath: status.podIP\n- name: CONTAINER CPU REQUEST MILLICORES\n  valueFrom:\n    resourceFieldRef:\n      resource: requests.cpu\n      divisor: lm\n```\n\n通过卷：\n\n```yml\nvolumes:\n- name: downward\n  downwardAPI:\n    items:\n    - path: \"podName\"\n      fieldRef:\n        fieldPath: metadata.name\n```\n\n![屏幕截图 2020-09-13 153906](/assets/屏幕截图%202020-09-13%20153906.png)\n\n### 使用 K8S API 服务器\n\nREST API：\n\n- 启动kubectl proxy\n\n```sh\ncurl http://localhost:8001/apis/batch/v1/jobs\n```\n\n在 pod 内部使用\n\n客户端API\n\n## Deployment\n\n更新应用：\n\n- 删除旧版本pod 启动新版本pod\n  - 会造成短暂的服务不可用\n- 启动新版本pod 删除旧版本pod\n\n![屏幕截图 2020-09-14 135712](/assets/屏幕截图%202020-09-14%20135712.png)\n\n### 使用rc进行滚动升级\n\n书上通过rolling-update的方法已经过时\n\n### 使用 Deployment 声明式升级\n\n- 创建\n\n```yml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - image: luksa/kubia:v1\n        name: nodejs\n```\n```sh\nkubectl create -f kubia-dep-v1.yaml --record # 加上该参数会记录历史版本号\n```\n\n- 更新版本\n\n```sh\nkubectl set image deployment kubia nodejs=luksa/kubia:v2\n```\n\n- 回滚\n\n```sh\nkubectl rollout undo deployment kubia\n```\n\n使用 - -to-revision=xxx 回滚到特定版本\n\n- 升级速率控制\n\n```yml\nrollingUpdate :\n  maxSurge: 1 # 最多允许超过的副本数\n  maxunavailable: 0 # 最多允许多少百分比pod不可用\n```\n\n- 使用rollout pause 暂停滚动升级 部分软件版本就不一样 金丝雀发布\n\n- minReadySeconds属性指定新创建的pod至少要成功运行多久之后 ， 才能 将其视为可用\n\n如果 一 个新的pod 运行出错， 并且在minReadySeconds时间内它的就绪探针出现了失败， 那么新版本的滚动升级将被阻止\n\n- 使用kubectl apply升级Deployment\n\n## StatefulSet\n\n如何复制有状态的pod？\n\nStatefulset 保证了pod在重新调度后保留它们的标识和状态\n\n每个pod都有专属于它的持久卷\n\nK8S保证不会有两个相同标识和持久卷的pod同时运行\n\n### 使用\n\n- 创建持久卷\n- 创建控制 Service\n\n```yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia\nspec:\n  clusterIP: None\n  selector:\n    app: kubia\n  ports:\n  - name: http\n    port: 80\n```\n\n- 创建StatefulSet\n\n```yml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kubia\nspec:\n  serviceName: kubia\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubia # has to match .spec.template.metadata.labels\n  template:\n    metadata:\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - name: kubia\n        image: luksa/kubia-pet\n        ports:\n        - name: http\n          containerPort: 8080\n        volumeMounts:\n        - name: data\n          mountPath: /var/data\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      resources:\n        requests:\n          storage: 1Mi\n      accessModes:\n      - ReadWriteOnce\n```\n\n- 使用一个 Service 来访问 Pod\n\n```yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia-public\nspec:\n  selector:\n    app: kubia\n  ports:\n  -  port: 80\n     targetPort: 8080\n```\n\n### 发现伙伴节点\n\n- 容器内部通过DNS SRV 记录\n\n## 安全\n\n### pod 使用宿主节点的Linux命名空间\n\n- 使用宿主节点的网络命名空间\n\n```yaml\nspec:\n  hostNetwork: true\n```\n\n- 使用宿主节点的端口而不使用宿主节点的网络命名空间\n\n![屏幕截图 2020-09-16 142921](/assets/屏幕截图%202020-09-16%20142921.png)\n\n如果使用hostport 一个节点只能有一个相同的pod\n\n- 使用宿主的PID与IPC空间\n\n```yml\nspec:\n  hostPID: true\n  hostIPC: true\n```\n\n开启后 相同节点的pod的进程之间就是可见的 可通信的\n\n### 安全上下文\n\n```yml\nspec:\n  securityContext:\n    # ... pod 级别的\n  containers:\n    securityContext:\n      runAsUser: 405 # 以指定用户运行\n      runAsNonRoot: true # 禁止以root运行\n      privileged: true # 在特权模式下允许\n      capabilities:\n        add:\n        - SYS_TIME # 开放硬件时间修改权限\n        drop:\n        - CHOWN # 禁用文件所有者修改权限\n      readOnlyRootFilesystem: true # 禁止在根目录写文件\n```\n\n### pod 网络隔离\n\n- 网络策略\n\n```yml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: postgres-netpolicy\nspec:\n  podSelector:\n    matchLabels:\n      app: database # 对该标签的pod生效\n  ingress: # 只允许来自匹配下面标签的pod请求\n  - from:\n    - podSelector:\n        matchLabels:\n          app: webserver\n    ports:\n    - port: 5432\n```\n\n## 计算资源管理\n\n### 申请资源\n\n```yaml\nspec:\n  containers:\n  - image: busybox\n    command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"]\n    name: main\n    resources:\n     requests:\n       cpu: 200m # 申请200毫核 也就说20%CPU\n       memory: 10Mi # 申请10M内存\n```\n\n添加了requests对调度的影响：\n\n通过设置资源requests我们指定了pod对资源需求的最小值。\n\n调度器不关心资源的实际使用了 而是关心各pod所定义的requests资源量 \n\n![屏幕截图 2020-09-17 134722](/assets/屏幕截图%202020-09-17%20134722.png)\n\n### 限制资源\n\n```yml\nresources:\n  limits:\n    cpu: 1 # 允许最大使用1核\n    memory: 20Mi # 内存允许最大 20M\n```\n\n超过limits的情况：\n\n- cpu：进程分配到的CPU不会超过指定的\n- 内存：如果内存超过limit 则容器会被杀掉\n\n### QoS 等级\n\n通过定义优先级决定资源不足时杀谁\n\n![屏幕截图 2020-09-17 142031](/assets/屏幕截图%202020-09-17%20142031.png)\n\n- BestEffort 优先级最低\n  - 没有设置requess和limits都属于这个级别\n- Guaranteed 优先级最高\n  - cpu和内存都要设置requests 和 limits\n  - 所有容器都要设置资源量\n  - requests 与 limits必须相等\n- Burstable 其他的pod都属于这个等级\n\n### 限制命名空间中的pod\n\n- LimitRange插件\n- ResourceQuota\n\n### 监控 pod\n\n- Heapster\n\n![屏幕截图 2020-09-17 143016](/assets/屏幕截图%202020-09-17%20143016.png)\n\n## 自动伸缩与集群\n\n- 基于CPU使用率的自动伸缩\n\n```sh\nkubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5\n```\n\n- 纵向扩容\n\n自动修改CPU与内存大小\n\n### 集群节点扩容\n\n新节点启动后，其上运行的Kubelet会联系API服务器，创建 一 个Node资源以注册该节点\n\n当一 个节点被选中下线，它首先会被标记为不可调度， 随后运行其上的pod 将被疏散至其他节点\n\n## 高级调度\n\n### 污点和容忍度\n\n限制哪些pod可以被调度到某 一 个节点\n\n```sh\nkubectl describe node minikube | grep Taints # 查看节点污点\n```\n\n![屏幕截图 2020-09-19 134744](/assets/屏幕截图%202020-09-19%20134744.png)\n\n- NoSchedule 表示如果 pod 没有容忍这些污点， pod 则不能被调度到包含这些污点的节点上\n- PreferNoSchedule 是 NoSchedule 的 一 个宽松的版本， 表示尽量阻止pod 被调度到这个节点上， 但是如果没有其他节点可以调度， pod 依然会被调度到这个节点上\n- NoExecute会影响正在节点上运行着的 pod 。 如果在 一 个节点上添加了 NoExecute 污点， 那些在该节点上运行着的pod, 如果没有容忍这个 NoExecute 污点， 将会从这个节点去除\n\n- 添加污点\n\n```sh\nkubectl taint node minikube node-type=production:NoSchedule\n```\n\n- pod添加容忍度\n\n```yml\nspec:\n  replicas: 5\n  template:\n    spec:\n      ...\n      tolerations:\n      - key: node-type\n        operator: Equal\n        value: production\n        effect: NoSchedule\n```\n\n### 节点亲缘性\n\n这种机制允许你通知 Kubemetes将 pod 只调度到某个几点子集上面\n\n```yml\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: gpu\n            operator: In\n            values:\n            - \"true\"\n```\n\n![屏幕截图 2020-09-19 142905](/assets/屏幕截图%202020-09-19%20142905.png)\n\n## 最佳实践\n\n![屏幕截图 2020-09-19 143453](/assets/屏幕截图%202020-09-19%20143453.png)\n\n### pod 的生命周期\n\n1. 应用必须意识到会被杀死或者重新调度\n    - ip与主机名会发生变化\n    - 使用卷解决数据写入问题\n2. 不断重启的pod不会被重新调度\n3. 固定顺序启动pod\n    - 使用init容器\n    - 应用要处理好其他依赖没有准备好的情况\n4. 生命周期钩子\n    - postStart\n    - preStop\n5. pod的关闭\n\n![屏幕截图 2020-09-19 145717](/assets/屏幕截图%202020-09-19%20145717.png)\n\n### 客户端请求处理\n\n1. pod启动时避免客户端连接断开\n    - 使用一个就绪探针来探测pod是否准备好接受请求了\n2. pod关闭时避免请求断开\n    - 停止接受新连接\n    - 等待所有请求完成\n    - 关闭应用\n\n### 让应用方便运行与管理\n\n1. 可管理的容器镜像\n    - 镜像太大难以传输 镜像太小会缺失很多工具\n2. 合理给镜像打标签\n    - 不要使用latest 使用具体版本号\n3. 使用多维度的标签\n4. 使用注解描述额外信息\n5. 使用/dev/termination-log 写入失败信息\n6. 日志\n    - 将日志打印到标准输出方便查看\n    - 集中式日志系统\n\n## 应用扩展\n\n### CRD对象\n\n- 创建\n\n```yml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: websites.extensions.example.com\nspec:\n  scope: Namespaced\n  group: extensions.example.com\n  version: v1\n  names:\n    kind: Website\n    singular: website\n    plural: websites\n```\n\n- 创建CRD实例\n\n```yml\napiVersion: extensions.example.com/v1\nkind: Website\nmetadata:\n  name: kubia\nspec:\n gitRepo: https://github.com/luksa/kubia-website-example.git\n```\n\n### 服务目录\n\n服务目录就是列出所有服务的目录。 用户可以浏览目录并自行设置目录中列出的服务实例","commitList":[{"date":"2021-10-08T12:45:10+08:00","author":"cjiping","message":"doc开发期 构建期调整","hash":"704efe7455b7fb2fbd81ec60099191ff982e639a"},{"date":"2021-10-07T19:38:58+08:00","author":"My","message":"init","hash":"4e0456332231ba0523aa526415f9982377358870"}],"hasMoreCommit":false,"totalCommits":0}